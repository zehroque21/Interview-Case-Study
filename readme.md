# PySpark Study Case

This project is a study case demonstrating the use of PySpark to process and analyze sales data. The aim is to showcase concepts of large-scale data manipulation using Apache Spark.

## Setting Up the Environment

To get started, you need to activate the virtual environment where PySpark and other necessary libraries are installed. Please follow the steps below:

1. **Activate the Virtual Environment**

   Navigate to the project directory and activate the virtual environment using the following command:

   ```bash
   source env/bin/activate
2. **Install Dependencies**

    After activating the virtual environment, you need to install the required dependencies. Use the following command to install all dependencies listed in the requirements.txt file:

    ```bash
    pip install -r requirements.txt
## **Database Structure**

below is an image that illustrates the relationship between the tables involved in this study case:

![Table Relation](docs/table-relation.drawio.png)

## **Main Notebook**

The main script for this study case can be found in the Jupyter Notebook. You can access it through the link below:

[Open the Main Notebook](data_pipeline_nb.ipynb)