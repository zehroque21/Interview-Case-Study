{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92013c40-ca6a-4ebf-b3fb-237a5ea18d44",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b34855a0-185d-4759-ba99-64456ae618fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stagging path\n",
    "source_path = \"source_files\"\n",
    "\n",
    "# path to sources\n",
    "tables = {\n",
    "    \"products\": {\n",
    "        \"path\": \"products (1).csv\",\n",
    "        \"PK\": \"ProductID\",  # Chave primária\n",
    "        \"FK\": []  # Lista de chaves estrangeiras associadas (vazia se não houver)\n",
    "    },\n",
    "    \"sales_order_detail\": {\n",
    "        \"path\": \"sales_order_detail.csv\",\n",
    "        \"PK\": \"SalesOrderDetailID\",  # Chave primária\n",
    "        \"FK\": [\"SalesOrderID\", \"ProductID\"]  # Supondo que o order_id seja uma chave estrangeira\n",
    "    },\n",
    "    \"sales_order_header\": {\n",
    "        \"path\": \"sales_order_header (1).csv\",\n",
    "        \"PK\": \"SalesOrderID\",  # Chave primária\n",
    "        \"FK\": [\"CustomerID\", \"SalesPersonID\"]  # Lista de chaves estrangeiras associadas (vazia se não houver)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258e7ea-2108-4644-ad66-17a8de404999",
   "metadata": {},
   "source": [
    "## Spark Session Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "911a5819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, udf, to_date\n",
    "from pyspark.sql.types import DateType, IntegerType\n",
    "from datetime import timedelta\n",
    "\n",
    "# create spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# show spark version\n",
    "print(spark.version)\n",
    "\n",
    "# Configurar o logger para ERROR - Spark Run A Lot of Error Running Locally \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f61885-54f1-47ce-8946-0e7860d743b5",
   "metadata": {},
   "source": [
    "## Raw Function [Data Loading]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "68dd0e6c-7469-4536-97cf-2fe6b058511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw(tables={}):\n",
    "    \"\"\" \n",
    "    1. Data Loading\n",
    "        Processes and saves tables from CSV files into Parquet format.\n",
    "\n",
    "    This function loops through a dictionary of tables, loads each CSV file, \n",
    "    checks for duplicates in the primary key (PK), and saves the table in \n",
    "    Parquet format.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    tables : dict\n",
    "        A dictionary where each key is the name of a table and the value is \n",
    "        another dictionary containing:\n",
    "        \n",
    "        - 'path': the path to the CSV file.\n",
    "        - 'PK': the name of the primary key column.\n",
    "    \"\"\"\n",
    "\n",
    "    # for each table in the parameter list\n",
    "    for table_name, table_params in tables.items():\n",
    "        \n",
    "        # create table path\n",
    "        table_path = f\"{source_path}/{table_params['path']}\"\n",
    "\n",
    "        # load csv using header, auto dectect schema from spark\n",
    "        stagging_table = spark.read.csv(table_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # grant unique PK\n",
    "        if stagging_table.groupBy(table_params['PK']).count().filter(col(\"count\") > 1).count() > 0:\n",
    "\n",
    "            # drop duplicates using PK - use first line\n",
    "            stagging_table = stagging_table.dropDuplicates(subset=[table_params['PK']])\n",
    "            # raise ValueError(f\"column '{table_params['PK']}' has duplicates.\")\n",
    "\n",
    "        # save raw table\n",
    "        stagging_table.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .parquet(f\"data/raw_{table_name}\", compression=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bfbf1817-d637-4064-9b30-f2a300718611",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec0d95-ca08-4474-a03d-f7bbb9c341ab",
   "metadata": {},
   "source": [
    "## Trusted Function [Data Review and Storage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5206f394-4c92-4e95-8b24-621e0439d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trusted(tables={}):\n",
    "    \"\"\" \n",
    "    2. Data Review and Storage\n",
    "        Processes and saves trusted tables from raw Parquet files.\n",
    "\n",
    "    This function iterates over a dictionary of tables, loads each raw \n",
    "    Parquet file, modifies the primary key (PK) and foreign key (FK) \n",
    "    columns by appending a suffix, and then saves the modified table \n",
    "    in a new Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    tables : dict\n",
    "        A dictionary where each key is the name of a table and the value is \n",
    "        another dictionary containing:\n",
    "        \n",
    "        - 'PK': the name of the primary key column.\n",
    "        - 'FK': a list of foreign key column names.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    The raw tables are expected to be stored under the 'data/raw_' prefix,\n",
    "    and the processed trusted tables will be saved with the 'data/store_' prefix.\n",
    "    \"\"\"\n",
    "\n",
    "    # for each table in the parameter list\n",
    "    for table_name, table_params in tables.items():\n",
    "\n",
    "        # load table\n",
    "        raw_path = f\"data/raw_{table_name}\"\n",
    "        raw_table = spark.read.parquet(raw_path)\n",
    "\n",
    "        # flag PK column\n",
    "        raw_table = raw_table.withColumnRenamed(table_params['PK'], table_params['PK']+'_PK')\n",
    "\n",
    "        # flag FK columns\n",
    "        for fk_col in table_params['FK']:\n",
    "            raw_table = raw_table.withColumnRenamed(fk_col, fk_col+'_FK')\n",
    "\n",
    "        # save store_ table \n",
    "        raw_table.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .parquet(f\"data/store_{table_name}\", compression=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "216156c1-96a9-42a2-a972-fde24abdfdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trusted(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400bd622-fbe1-4c0e-aa72-a0d21839f772",
   "metadata": {},
   "source": [
    "## Refined Function [Transformations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b8ed3f7c-4009-4e3b-981a-862e271bbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refined_products():\n",
    "    \"\"\" \n",
    "    3. Product Master Transformations:\n",
    "        Processes and refines product data by loading from a Parquet file, \n",
    "    replacing NULL values, and enhancing the product category.\n",
    "\n",
    "    The function performs the following operations:\n",
    "    \n",
    "    - Loads a product table from Parquet format.\n",
    "    - Replaces NULL values in the \"Color\" field with \"N/A\".\n",
    "    - Enhances the \"ProductCategoryName\" field\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    - The source data is expected to be located at `data/store_products`.\n",
    "    - The refined product data will be saved to `data/publish_product`.\n",
    "    \"\"\"\n",
    "\n",
    "    products_table = 'products'\n",
    "\n",
    "    # load table\n",
    "    refined_path = f\"data/store_{products_table}\"\n",
    "    products_df = spark.read.parquet(refined_path)\n",
    "\n",
    "    # i. \n",
    "    # Replace NULL values in the Color field with N/A.\n",
    "    products_df = products_df.fillna({'Color': 'N/A'})\n",
    "\n",
    "    # ii. \n",
    "    # Enhance the ProductCategoryName field when it is NULL using the\n",
    "    # following logic:\n",
    "    products_df = products_df.withColumn(\"ProductCategoryName\",\n",
    "        when(\n",
    "            col(\"ProductCategoryName\").isNull() \n",
    "            & col(\"ProductSubCategoryName\").isin(\"Gloves\", \"Shorts\", \"Socks\", \"Tights\", \"Vests\"), \n",
    "        \"Clothing\")\n",
    "        .when(\n",
    "            col(\"ProductCategoryName\").isNull() \n",
    "            & col(\"ProductSubCategoryName\").isin(\"Locks\", \"Lights\", \"Headsets\", \"Helmets\", \"Pedals\", \"Pumps\"), \n",
    "        \"Accessories\")\n",
    "        .when(\n",
    "            col(\"ProductCategoryName\").isNull() \n",
    "            & (col(\"ProductSubCategoryName\").contains(\"Frames\") | col(\"ProductSubCategoryName\").isin(\"Wheels\", \"Saddles\")), \n",
    "        \"Frames\")\n",
    "        .otherwise(col(\"ProductCategoryName\")) # Still has NULLS !!!\n",
    "    )\n",
    "\n",
    "    # save refined table\n",
    "    products_df.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .parquet(\"data/publish_product\", compression=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "594fc7de-19bc-44b6-b5ec-d366a078f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_products()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0eefc975-6fa0-4656-b634-53690d545761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refined_salesOrders():\n",
    "    \"\"\"\n",
    "        Processes and refines sales orders by loading data from parquet files,\n",
    "    calculating additional fields, and saving the results into a new parquet file.\n",
    "\n",
    "    The function performs the following operations:\n",
    "\n",
    "    1. Loads sales order header and detail data from parquet files.\n",
    "    2. Joins the header and detail DataFrames on the SalesOrderID columns.\n",
    "    3. Calculates the lead time in business days between OrderDate and ShipDate, \n",
    "       excluding Saturdays and Sundays.\n",
    "    4. Calculates the TotalLineExtendedPrice based on the formula:\n",
    "       OrderQty * (UnitPrice - UnitPriceDiscount).\n",
    "    5. Creates a new DataFrame with selected columns, including new calculated columns.\n",
    "    6. Renames the \"Freight\" column to \"TotalOrderFreight\".\n",
    "    7. Writes the refined DataFrame to a parquet file named \"publish_orders\".\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The parquet data source is expected to be located in the \"data\" directory\n",
    "      with filenames following the pattern \"store_sales_order_header\" and \n",
    "      \"store_sales_order_detail\".\n",
    "    - The output will be saved as a parquet file named \"publish_orders\" in the \"data\" directory.\n",
    "    - This function utilizes a custom user-defined function (UDF) to calculate\n",
    "      the lead time in business days.\n",
    "    \"\"\"\n",
    "\n",
    "    # source table names\n",
    "    header_table = 'sales_order_header'\n",
    "    detail_table = 'sales_order_detail'\n",
    "\n",
    "\n",
    "    # load tables \n",
    "    orderHeader_df = spark.read.parquet(f'data/store_{header_table}')\n",
    "    orderDetail_df = spark.read.parquet(f'data/store_{detail_table}')\n",
    "\n",
    "    # join header and details as inner getting only headers that have details attached\n",
    "    orders_df = orderHeader_df.join(\n",
    "        orderDetail_df, \n",
    "        orderDetail_df.SalesOrderID_FK==orderHeader_df.SalesOrderID_PK, \n",
    "        how='inner')\n",
    "\n",
    "    # i. \n",
    "    # Calculate LeadTimeInBusinessDays as the difference between\n",
    "    # OrderDate and ShipDate, excluding Saturdays and Sundays\n",
    "\n",
    "    # Function to calculate business days\n",
    "    def days_diff_only_workdays(start_dt, end_dt):\n",
    "\n",
    "        # start counter and start date space\n",
    "        c_day = 0\n",
    "        current_dt = start_dt\n",
    "\n",
    "        # add one day until get the end date\n",
    "        while current_dt <= end_dt:\n",
    "\n",
    "            # check if day is between monday and friday \n",
    "            if current_dt.weekday() < 5:  \n",
    "                c_day += 1\n",
    "            current_dt += timedelta(days=1)\n",
    "\n",
    "        # return the number of counted days that are weekdays\n",
    "        return c_day\n",
    "    \n",
    "    # Register User Function\n",
    "    days_diff_only_workdays_udf = udf(days_diff_only_workdays, IntegerType())\n",
    "    \n",
    "    # create new column with using custom function\n",
    "    orders_df = orders_df.withColumn(\n",
    "        \"LeadTimeInBusinessDays\", \n",
    "        days_diff_only_workdays_udf(to_date(col('OrderDate')), to_date(col('ShipDate'))))\n",
    "\n",
    "    # ii. \n",
    "    # Calculate TotalLineExtendedPrice using the formula: \n",
    "    # OrderQty * (UnitPrice - UnitPriceDiscount)\n",
    "\n",
    "    # create new TotalLineExtendedPrice column based on the formula\n",
    "    orders_df = orders_df.withColumn(\n",
    "        'TotalLineExtendedPrice', \n",
    "        col('OrderQty')*(col('UnitPrice') - col('UnitPriceDiscount')))\n",
    "\n",
    "    # iii. \n",
    "    # Write the results into a table named publish_orders\n",
    "    \n",
    "    # create table shape selecting columns\n",
    "    new_refined_columns = ['LeadTimeInBusinessDays', 'TotalLineExtendedPrice']\n",
    "    \n",
    "    # get all columns from detail, some columns from header and refined columns\n",
    "    orders_df = orders_df.select(\n",
    "        orderDetail_df.columns +\n",
    "        [c for c in orderHeader_df.columns if c not in ['SalesOrderId']] +\n",
    "        new_refined_columns)\n",
    "\n",
    "    # rename Freight column\n",
    "    orders_df = orders_df.withColumnRenamed(\"Freight\", \"TotalOrderFreight\")\n",
    "\n",
    "    # save refined table\n",
    "    orders_df.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .parquet(\"data/publish_orders\", compression=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3be85ff0-f65f-4542-a8e9-d9953d16e865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "refined_salesOrders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaad495-b34e-4f68-86f4-ddb4061409e2",
   "metadata": {},
   "source": [
    "## Analysis Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9199bbbf-351e-413a-b7b0-b96c436c4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sql tmp table for analysis\n",
    "\n",
    "spark.read.parquet('data/publish_orders').createOrReplaceTempView(\"orders\")\n",
    "spark.read.parquet('data/publish_product').createOrReplaceTempView(\"product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616dee6-5426-4e12-8ee1-7f9b07755e7a",
   "metadata": {},
   "source": [
    "### i. Which color generated the highest revenue each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "755c0321-ac33-48b7-8672-b360b37f6950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------------------------------+\n",
      "|year(OrderDate)| Color|sum_TotalLineExtendedPrice_by_year|\n",
      "+---------------+------+----------------------------------+\n",
      "|           2023| Black|              1.5047694369201014E7|\n",
      "|           2022| Black|              1.4005242975200394E7|\n",
      "|           2022|   Red|               1.156549116229977E7|\n",
      "|           2023|Yellow|              1.0638314918100433E7|\n",
      "|           2023|Silver|                  7613990.73660019|\n",
      "|           2024|Yellow|                 6368158.478900213|\n",
      "|           2021|   Red|                 6019614.015699884|\n",
      "|           2023|  Blue|                 5966277.821900176|\n",
      "|           2022|Silver|                 5726533.221599957|\n",
      "|           2024| Black|                 5579326.790800353|\n",
      "+---------------+------+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select year(OrderDate),\n",
    "        Color,\n",
    "        sum(TotalLineExtendedPrice) sum_TotalLineExtendedPrice_by_year\n",
    "        from orders A\n",
    "    join product B\n",
    "    on A.ProductID_FK = B.ProductID_PK\n",
    "    group by 1, 2\n",
    "    order by 3 desc, 1\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a01126-99b7-4fb8-b03c-6c8ede3ebc43",
   "metadata": {},
   "source": [
    "### ii. What is the average LeadTimeInBusinessDays by ProductCategoryName?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "db08792c-d980-4181-8efa-fc1cb6c829cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------------+\n",
      "|ProductCategoryName|avg_LeadTimeInBusinessDays|\n",
      "+-------------------+--------------------------+\n",
      "|           Clothing|         5.711666367068129|\n",
      "|        Accessories|         5.702787804316105|\n",
      "|             Frames|         5.685900314324203|\n",
      "|              Bikes|         5.667897567632656|\n",
      "|         Components|         5.655127960275019|\n",
      "+-------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select ProductCategoryName,\n",
    "        mean(LeadTimeInBusinessDays) avg_LeadTimeInBusinessDays\n",
    "        from orders A\n",
    "    join product B\n",
    "    on A.ProductID_FK = B.ProductID_PK\n",
    "    where ProductCategoryName is not null\n",
    "    group by 1\n",
    "    order by 2 Desc\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18f67d-1cd9-4fe6-a340-6f29379020e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
